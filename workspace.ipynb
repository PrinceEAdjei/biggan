{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "from models import *\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "!nvidia-smi | head -19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class progGAN(BaseModel):\n",
    "    ''''''\n",
    "    def __init__(self, size):\n",
    "        ''''''\n",
    "        self.size = size\n",
    "        self.G = NVIDIA_generator(size)\n",
    "        self.D = NVIDIA_discriminator(size)\n",
    "        super(BaseModel, self).__init__(\n",
    "            [i for m in (self.G, self.D) for i in m.inputs],\n",
    "            [o for m in (self.G, self.D) for o in m.outputs])      \n",
    "    \n",
    "    def train(self, input_dir, output_dir='./out', batch_size=1, period=100., lambda_GP=10, \n",
    "              lr=1e-3, n_critic=1, gamma=750, epsilon_drift=0.001, summary_every=np.inf):\n",
    "        ''''''\n",
    "        print('Building inputs')\n",
    "        with tf.variable_scope('Inputs'):\n",
    "            coord = tf.train.Coordinator()\n",
    "            z = tf.random_normal(shape=(batch_size, 512))\n",
    "            x = self.stream_input([input_dir], self.size, batch_size)\n",
    "            #x = tf.stack([tf.random_crop(x, [self.size, self.size, 3]) for _ in range(batch_size)], 0)\n",
    "            xs = [tf.image.resize_bilinear(x, [s, s]) for s in [\n",
    "                2**(2+i) for i in range(int(np.log2(self.size//4))+1)]]\n",
    "            xs_hat = self.G(z)\n",
    "        \n",
    "        print('Building losses')\n",
    "        with tf.variable_scope('Losses'):\n",
    "            \n",
    "            step = tf.Variable(0, name='step')\n",
    "            tooth, blend, quantize = self.blend(step, period=period, n_scales=len(xs))\n",
    "            \n",
    "            #xs_hat = self.residual(xs_hat, tooth)\n",
    "            real = self.D(xs)\n",
    "            fake = self.D(xs_hat)\n",
    "            GP = self.grad_penalty(xs, xs_hat)\n",
    "            \n",
    "            real_mask = tf.reduce_mean(tf.reduce_sum(real * blend, axis=-1))\n",
    "            fake_mask = tf.reduce_mean(tf.reduce_sum(fake * blend, axis=-1))\n",
    "            \n",
    "            GP_mask = tf.reduce_mean(tf.reduce_sum(GP * blend, axis=-1))\n",
    "            drift_mask = tf.reduce_mean(tf.reduce_sum(real**2 * blend, axis=-1))\n",
    "            \n",
    "            L_G = -fake_mask\n",
    "            L_D = -real_mask + fake_mask \n",
    "            L_D_tot = L_D + epsilon_drift*drift_mask + lambda_GP*GP_mask\n",
    "            # TODO: add feature regularizer\n",
    "        \n",
    "        print('Building optimizers')\n",
    "        with tf.variable_scope('Optimizers'):\n",
    "            G_opt = tf.train.AdamOptimizer(lr, beta1=0, beta2=0.99).minimize(\n",
    "                L_G, var_list=self.G.trainable_weights, global_step=step)\n",
    "            D_opt = tf.train.AdamOptimizer(lr, beta1=0, beta2=0.99).minimize(\n",
    "                L_D_tot, var_list=self.D.trainable_weights)\n",
    "            \n",
    "        print('Building summary')\n",
    "        with tf.variable_scope('Summary'):\n",
    "            img_dict={\n",
    "                'real_'+str(self.G.output_shape[i][1]): self.postproc_img(x) \\\n",
    "                for i, x in enumerate(xs)}\n",
    "            fake_img_dict={\n",
    "                'fake_'+str(self.G.output_shape[i][1]): self.postproc_img(x) \\\n",
    "                for i, x in enumerate(xs_hat)}\n",
    "            img_dict.update(fake_img_dict)\n",
    "            scalar_dict={\n",
    "                'L_D_tot': L_D_tot,\n",
    "                'L_D': L_D, \n",
    "                'L_G': L_G,\n",
    "                'GP_mask': GP_mask,\n",
    "                'drift_mask': drift_mask}\n",
    "        summary, writer = self.make_summary(output_dir, \n",
    "            img_dict=img_dict, scalar_dict=scalar_dict, graph=None)\n",
    "        \n",
    "        print('Initializing')\n",
    "        with tf.Session() as sess:\n",
    "            # TODO: scale weights at runtime (section 4.1)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "            self.graph.finalize()\n",
    "            print('Start of training'); time.sleep(1); n = 0\n",
    "            try:\n",
    "                while not coord.should_stop():\n",
    "                    for i in tqdm.trange(10000, disable=True):\n",
    "                        for _ in range(n_critic):\n",
    "                            sess.run(D_opt)\n",
    "                        if n % summary_every:\n",
    "                            n = sess.run([G_opt, step])[1]\n",
    "                        else: \n",
    "                            s, n = sess.run([G_opt, summary, step])[1:]\n",
    "                            writer.add_summary(s, n)\n",
    "                            writer.flush()\n",
    "            except:\n",
    "                coord.request_stop()\n",
    "                time.sleep(1)\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg = progGAN(32)\n",
    "print('Gen params ~ {:0.1e}\\nDisc params ~ {:0.1e}'.format(\n",
    "    pg.G.count_params(), pg.D.count_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pg.train(\n",
    "    input_dir='/data/datasets/celeba/img_align_celeba/', \n",
    "    output_dir='out/tooth',#'./out/celeba_32_period100k_edrift0.01_QUANTIZE', \n",
    "    batch_size=8, summary_every=10,\n",
    "    period=1000, n_critic=1, lr=1e-3, \n",
    "    lambda_GP=10, gamma=750, epsilon_drift=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -r out/tooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tri(s):\n",
    "    scale = arange(8)\n",
    "    init = clip(scale+2-s, 0, 1)\n",
    "    tri = 1 - clip(abs(scale + 1 - s), 0, 1)\n",
    "    #ramp = maximum(1 - (s - scale - 1), 0)\n",
    "    #tooth = ramp * (1 - greater(ramp, 1).astype(float))\n",
    "    return concatenate([init[0:1],  tri[1:]])\n",
    "imshow([tri(s) for s in linspace(0, 8, 100)], aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
